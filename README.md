# Обработка видеофиксации транспорта MIET Code

## Описание файлов
```MIET_Code_presentation.pptx``` - файл с презентацией решения

```model2.ipynb``` - ноутбук для одиночного инференса с отрисовкой зон и детекцией скорости

```model2_nogui.ipynb``` - ноутбук для инференса на тестовых данных

## Гайд по запуску
Скачиваем ```model2.ipynb```. 

Здесь меняем пути к файлам ```.mp4``` и ```.json```
```
video_path=r"C:\Users\Lexie\Desktop\Минприроды Видеофиксация\Датасет_для_обучения\Video1\KRA-2-7-2023-08-22-morning.mp4"
markup=r"C:\Users\Lexie\Desktop\Минприроды Видеофиксация\Датасет_для_обучения\markup\jsons\KRA-2-7-2023-08-22-morning.json"
```
Запускаем ноутбук и смотрим на красивую детекцию)
## Наше решение 
### Что успели сделать
Взяли готовую модель ```YOLOv8n```. К ней дописали алгоритм для детекции скорости: Если точка трекера объекта, прикреплённая к нижней части bbox пересекает первую  зону(поля 'zones' из json-файлов датасета), то bbox этого объекта становится bboxом 'интереса'. Затем, если трекер пересекает вторую зону, то мы начинаем рассчёт скорости с учётом расстояния в 20м. Если трекер не пересекает вторую зону, значит объект выехал из интересующей нас area в процессе движения, и скорость для такого объекта мы не считаем.

![Frame 1](https://github.com/Nikevich/Hak_MIET_Code/assets/111390447/056cb845-74eb-4934-ad30-0032bfd61a90)
В модель напрямую подаются сразу данные для inference и подготовки результата в виде ```.csv``` файла
![изображение](https://github.com/Nikevich/Hak_MIET_Code/assets/111390447/045cc763-b62f-43a8-9db7-2112b8452881)


### Что не успели
Из каждого видео в датасете мы берём 40 кадров. Затем эти кадры подаются в модель [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO). Эта модель детектирует классы на изображении по текстовым промптам(эти промпты - это названия нужных нам классов). Затем мы подаём размеченные изображения для дообученные ```YOLOv8s```. После этого мы повторяем процедру для inference

![Frame 2](https://github.com/Nikevich/Hak_MIET_Code/assets/111390447/f050a325-7fd6-4958-9e00-367c9a29f92e)

Так же мы хотим аугментировать наши данные с помощью библиотеки [Albumentations](https://github.com/albumentations-team/albumentations). В качестве аугментаций планируем использовать наложение 'снега','дождя' и 'теней' на изображения. Это позовлить улучшить обобщаущую способность модели и её станет возможно использовать в том числе во время погодных условиях. 

![Grou (2)](https://github.com/Nikevich/Hak_MIET_Code/assets/111390447/83c8b8fe-3b7a-422e-a86e-191179fd5175)

**По сути, все вышеописанные шаги мы проделали. Мы разметили данные с помощью Grounding DINO. Мы даже успели применить аугментации с помощью Albumentations. Но по какой-то причине размеченные данные сохранились не в том формате, который распознаёт YOLO модель. И у нас не хватило времени для того, чтобы преобразовать датасет в нужный формат, поэтому в качестве решения используется 'голая' YOLO без дообучения. Описание соответствующих ноутбуков:**

```Augmentation.ipynb``` - файл для аугментации кадров с помощью Albumentations

```Crop.ipynb``` - файл для нарезки видео на кадры 

```AugmentationAndCoco.ipynb``` - файл для разметки с помощью Grounding DINO
